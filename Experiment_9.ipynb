{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experiment 9: Handling Imbalanced Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:Implement techniques to handle imbalanced datasets and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Dataset import and Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "\n",
    "# 1. Dataset Import and Exploration\n",
    "# Load your dataset\n",
    "df = pd.read_csv('imbalanced_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Show basic details about the datasete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   feature_1      1000 non-null   float64\n",
      " 1   feature_2      1000 non-null   float64\n",
      " 2   feature_3      1000 non-null   float64\n",
      " 3   feature_4      1000 non-null   float64\n",
      " 4   feature_5      1000 non-null   float64\n",
      " 5   feature_6      1000 non-null   float64\n",
      " 6   feature_7      1000 non-null   float64\n",
      " 7   feature_8      1000 non-null   float64\n",
      " 8   feature_9      1000 non-null   float64\n",
      " 9   feature_10     1000 non-null   float64\n",
      " 10  target_column  1000 non-null   int64  \n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 86.1 KB\n",
      "None\n",
      "\n",
      "Class Distribution:\n",
      "target_column\n",
      "0    900\n",
      "1    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Info:\")\n",
    "print(df.info()) # Data types and missing values\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['target_column'].value_counts()) # Replace 'target_column' with your actual target column name\n",
    "\n",
    "# Save original dataset to CSV in the working directory\n",
    "df.to_csv('original_dataset.csv', index=False)\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = df.drop(columns=['target_column']) # Replace 'target_column' with your actual target column name\n",
    "y = df['target_column'] # Replace 'target_column' with your actual target column name\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Techniques for handling imbalanced data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Oversampling (Increase instances of the minority class by duplicating samples)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Random Under Sampling (Reduce instances of the majority class by randomly removing samples)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled_undersample, y_train_resampled_undersample =rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled_smote, y_train_resampled_smote =smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Save the resampled datasets to CSV in the working directory\n",
    "pd.DataFrame(X_train_resampled).to_csv('oversampled_data.csv', index=False)\n",
    "pd.DataFrame(X_train_resampled_undersample).to_csv('undersampled_data.csv',index=False)\n",
    "pd.DataFrame(X_train_resampled_smote).to_csv('smote_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Scaling the Data (StandardScaler)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_resampled_scaled = scaler.transform(X_test)\n",
    "X_train_resampled_undersample_scaled =scaler.fit_transform(X_train_resampled_undersample)\n",
    "X_test_resampled_undersample_scaled = scaler.transform(X_test)\n",
    "X_train_resampled_smote_scaled =scaler.fit_transform(X_train_resampled_smote)\n",
    "X_test_resampled_smote_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5:  Classifier Evaluation - Logistic Regression with Increased max_iter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Dataset Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       269\n",
      "           1       1.00      0.03      0.06        31\n",
      "\n",
      "    accuracy                           0.90       300\n",
      "   macro avg       0.95      0.52      0.50       300\n",
      "weighted avg       0.91      0.90      0.86       300\n",
      "\n",
      "AUC-ROC: 0.6764600071951073\n",
      "\n",
      "Random Oversampling Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.70      0.80       269\n",
      "           1       0.17      0.55      0.26        31\n",
      "\n",
      "    accuracy                           0.68       300\n",
      "   macro avg       0.55      0.62      0.53       300\n",
      "weighted avg       0.85      0.68      0.74       300\n",
      "\n",
      "AUC-ROC: 0.7061997841467802\n",
      "\n",
      "Random Under Sampling Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.71      0.80       269\n",
      "           1       0.17      0.52      0.25        31\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.55      0.61      0.53       300\n",
      "weighted avg       0.85      0.69      0.75       300\n",
      "\n",
      "AUC-ROC: 0.6822160930567214\n",
      "\n",
      "SMOTE Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83       269\n",
      "           1       0.20      0.58      0.30        31\n",
      "\n",
      "    accuracy                           0.72       300\n",
      "   macro avg       0.57      0.66      0.56       300\n",
      "weighted avg       0.86      0.72      0.77       300\n",
      "\n",
      "AUC-ROC: 0.7119558700083943\n"
     ]
    }
   ],
   "source": [
    "Solver = 'liblinear'\n",
    "# Initialize Logistic Regression with solver and max_iter\n",
    "model = LogisticRegression(solver='liblinear', max_iter=500) # You can adjust max_iter if needed\n",
    "\n",
    "# Train the model on the original imbalanced data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance on the original dataset\n",
    "print(\"\\nOriginal Dataset Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:,1]))\n",
    "\n",
    "# Train and evaluate on resampled data - Random Oversampling\n",
    "model.fit(X_train_resampled_scaled, y_train_resampled)\n",
    "y_pred_resampled = model.predict(X_test_resampled_scaled)\n",
    "\n",
    "print(\"\\nRandom Oversampling Performance:\")\n",
    "print(classification_report(y_test, y_pred_resampled))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test,model.predict_proba(X_test_resampled_scaled)[:, 1]))\n",
    "\n",
    "# Train and evaluate on resampled data - Random Under Sampling\n",
    "model.fit(X_train_resampled_undersample_scaled,y_train_resampled_undersample)\n",
    "y_pred_resampled_undersample =model.predict(X_test_resampled_undersample_scaled)\n",
    "\n",
    "print(\"\\nRandom Under Sampling Performance:\")\n",
    "print(classification_report(y_test, y_pred_resampled_undersample))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test,model.predict_proba(X_test_resampled_undersample_scaled)[:, 1]))\n",
    "\n",
    "# Train and evaluate on resampled data - SMOTE\n",
    "model.fit(X_train_resampled_smote_scaled, y_train_resampled_smote)\n",
    "y_pred_resampled_smote = model.predict(X_test_resampled_smote_scaled)\n",
    "\n",
    "print(\"\\nSMOTE Performance:\")\n",
    "print(classification_report(y_test, y_pred_resampled_smote))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test,model.predict_proba(X_test_resampled_smote_scaled)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Class Weighting - Modify the model to assign higher importance to the minority class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Weighting Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.71      0.80       269\n",
      "           1       0.17      0.52      0.25        31\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.55      0.61      0.53       300\n",
      "weighted avg       0.85      0.69      0.75       300\n",
      "\n",
      "AUC-ROC: 0.7083583163448854\n"
     ]
    }
   ],
   "source": [
    "model_with_weights = LogisticRegression(solver='liblinear',\n",
    "max_iter=500, class_weight='balanced')\n",
    "model_with_weights.fit(X_train_scaled, y_train)\n",
    "y_pred_with_weights = model_with_weights.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClass Weighting Performance:\")\n",
    "print(classification_report(y_test, y_pred_with_weights))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test,\n",
    "model_with_weights.predict_proba(X_test_scaled)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
